## socialMedia data
socialMedia = read.csv(paste0(path, '0_facebook/tps2_facebook.csv'),na.strings=c("","NA"), stringsAsFactors = FALSE)[-1,]
socialMedia = socialMedia %>%
mutate(TPS_ID = as.factor(TPS_ID),
OpenF = as.factor(OpenF)) %>%
mutate_if(is.character,as.numeric) %>%
mutate(fhab = rowMeans(.[,10:19], na.rm = FALSE),
femo = rowMeans(.[c(8,9,20:39)], na.rm = FALSE),
fconnected = rowMeans(.[c(8,9)], na.rm = FALSE),
femopost = rowMeans(.[c(20:39)], na.rm = FALSE),
TPS_ID = as.character(TPS_ID),
OpenF = as.character(OpenF)) %>%
select(TPS_ID, fhab, femo,fconnected,femopost=)
cyber = read.csv(paste0(path, '0_cyberball/cyberball_roi.csv'), stringsAsFactors = FALSE)
age = read.csv(paste0(path, '0_surveys/tps12_scanner_study_age.csv'), stringsAsFactors = FALSE) %>%
select(TPS_ID, age, study_t, scanner_t)
iri = read.csv(paste0(path, '0_surveys/iri_tps12.csv'), stringsAsFactors = FALSE)
iri = iri %>%
mutate(IRI_mean = rowMeans(select(iri, starts_with("IRI_")), na.rm = FALSE))
nts = read.csv(paste0(path, '0_surveys/nts.csv'), stringsAsFactors = FALSE)
df = cyber %>%
left_join(socialMedia, by = "TPS_ID") %>%
left_join(age, by = "TPS_ID") %>%
left_join(iri, by = "TPS_ID") %>%
left_join(nts, by = "TPS_ID")
# Remove NAs
df = df %>% drop_na(fhab,femo, NTS_Belongingness,
NTS_SelfEsteem, NTS_Control,
NTS_MeaningfulExistence, study_t, scanner_t,
IRI_Perspective_Taking, IRI_Fantasy,
IRI_Empathic_Concern, IRI_Personal_Distress,
fconnected) %>%
mutate(NTS_Belongingness = 7 - NTS_Belongingness,
NTS_SelfEsteem = 7 - NTS_SelfEsteem,
NTS_Control = 7 - NTS_Control,
NTS_MeaningfulExistence = 7 - NTS_MeaningfulExistence,
NTS_Mean = 7 - NTS_Mean)
df[2,"OpenF"]
df$OpenF
rm(list = ls())
if (!require("pacman")) install.packages("pacman",repos = "http://cran.us.r-project.org")
pacman::p_load(ggplot2,ggpubr,tidyverse,plyr, dplyr, knitr, kableExtra, jtools,pander)
source("~/Documents/GitHub/facebookUse/functions.R")
pd <- position_dodge(0.3) # move them .05 to the left and right
path =  "~/Box Sync/CurrentProjects_Penn/TPS12_BART/99_fomo/github/"
setwd(path)
## socialMedia data
socialMedia = read.csv(paste0(path, '0_facebook/tps2_facebook.csv'),na.strings=c("","NA"), stringsAsFactors = FALSE)[-1,]
socialMedia = socialMedia %>%
mutate(TPS_ID = as.factor(TPS_ID),
OpenF = as.factor(OpenF)) %>%
mutate_if(is.character,as.numeric) %>%
mutate(fhab = rowMeans(.[,10:19], na.rm = FALSE),
femo = rowMeans(.[c(8,9,20:39)], na.rm = FALSE),
fconnected = rowMeans(.[c(8,9)], na.rm = FALSE),
femopost = rowMeans(.[c(20:39)], na.rm = FALSE),
TPS_ID = as.character(TPS_ID),
OpenF = as.character(OpenF)) %>%
select(TPS_ID, fhab, femo,fconnected,femopost, OpenF)
cyber = read.csv(paste0(path, '0_cyberball/cyberball_roi.csv'), stringsAsFactors = FALSE)
age = read.csv(paste0(path, '0_surveys/tps12_scanner_study_age.csv'), stringsAsFactors = FALSE) %>%
select(TPS_ID, age, study_t, scanner_t)
iri = read.csv(paste0(path, '0_surveys/iri_tps12.csv'), stringsAsFactors = FALSE)
iri = iri %>%
mutate(IRI_mean = rowMeans(select(iri, starts_with("IRI_")), na.rm = FALSE))
nts = read.csv(paste0(path, '0_surveys/nts.csv'), stringsAsFactors = FALSE)
df = cyber %>%
left_join(socialMedia, by = "TPS_ID") %>%
left_join(age, by = "TPS_ID") %>%
left_join(iri, by = "TPS_ID") %>%
left_join(nts, by = "TPS_ID")
# Remove NAs
df = df %>% drop_na(fhab,femo, NTS_Belongingness,
NTS_SelfEsteem, NTS_Control,
NTS_MeaningfulExistence, study_t, scanner_t,
IRI_Perspective_Taking, IRI_Fantasy,
IRI_Empathic_Concern, IRI_Personal_Distress,
fconnected) %>%
mutate(NTS_Belongingness = 7 - NTS_Belongingness,
NTS_SelfEsteem = 7 - NTS_SelfEsteem,
NTS_Control = 7 - NTS_Control,
NTS_MeaningfulExistence = 7 - NTS_MeaningfulExistence,
NTS_Mean = 7 - NTS_Mean)
df[2,"OpenF"]
getWC_mat = function(text, w){
docs <- Corpus(VectorSource(text))
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
# docs <- tm_map(docs, stemDocument)
## Build a term-document matrix
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v*w)
}
getWC_mat(df[2,"OpenF"],3)
getWC_mat = function(text, w){
docs <- Corpus(VectorSource(text))
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
# docs <- tm_map(docs, stemDocument)
## Build a term-document matrix
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v*w)
return(d)
}
getWC_mat(df[2,"OpenF"],3)
getWC_mat(df$OpenF[2], df$fhab[2])
getWC_mat = function(text, w){
docs <- Corpus(VectorSource(text))
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
# docs <- tm_map(docs, stemDocument)
## Build a term-document matrix
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v*w)
return(d)
}
total_wc = getWC_mat(df$OpenF[1], df$fhab[1])
total_wc = getWC_mat(df$OpenF[1], df$fhab[1])
for (i in c(2:dim(df)[1])){
total_wc = merge(total_wc, getWC_mat(df$OpenF[2], df$fhab[2]), by = "word")
}
warnings()
total_wc = getWC_mat(df$OpenF[1], df$fhab[1])
for (i in c(2:dim(df)[1])){
wc = getWC_mat(df$OpenF[2], df$fhab[2])
colnames(wc)[2] = paste0("col",i)
total_wc = merge(total_wc, , by = "word")
}
total_wc = getWC_mat(df$OpenF[1], df$fhab[1])
for (i in c(2:dim(df)[1])){
wc = getWC_mat(df$OpenF[2], df$fhab[2])
colnames(wc)[2] = paste0("col",i)
total_wc = merge(total_wc, wc, by = "word")
}
total_wc
for (i in c(2:dim(df)[1])){
wc = getWC_mat(df$OpenF[i], df$fhab[i])
colnames(wc)[2] = paste0("col",i)
total_wc = merge(total_wc, wc, by = "word")
}
total_wc = getWC_mat(df$OpenF[1], df$fhab[1])
for (i in c(2:dim(df)[1])){
wc = getWC_mat(df$OpenF[i], df$fhab[i])
colnames(wc)[2] = paste0("col",i)
total_wc = merge(total_wc, wc, by = "word")
}
total_wc
colnames(wc)
total_wc = getWC_mat(df$OpenF[1], df$fhab[1])
for (i in c(2:dim(df)[1])){
wc = getWC_mat(df$OpenF[i], df$fhab[i])
colnames(wc)[2] = paste0("col",i)
total_wc = merge(total_wc, wc, by = "word", all = TRUE)
}
total_wc
dim(total_wc)
colnames(wc)[2] = "col1"
total_wc = getWC_mat(df$OpenF[1], df$fhab[1])
colnames(wc)[2] = "col1"
for (i in c(2:dim(df)[1])){
wc = getWC_mat(df$OpenF[i], df$fhab[i])
colnames(wc)[2] = paste0("col",i)
total_wc = merge(total_wc, wc, by = "word", all = TRUE)
}
total_wc$freq
total_wc = getWC_mat(df$OpenF[1], df$fhab[1])
colnames(wc)[2] = "col1"
for (i in c(2:dim(df)[1])){
wc = getWC_mat(df$OpenF[i], df$fhab[i])
colnames(wc)[2] = paste0("col",i)
total_wc = merge(total_wc, wc, by = "word", all = TRUE)
}
total_wc$freq
total_wc = getWC_mat(df$OpenF[1], df$fhab[1])
colnames(wc)[2] = "col1"
total_wc = getWC_mat(df$OpenF[1], df$fhab[1])
colnames(total_wc)[2] = "col1"
for (i in c(2:dim(df)[1])){
wc = getWC_mat(df$OpenF[i], df$fhab[i])
colnames(wc)[2] = paste0("col",i)
total_wc = merge(total_wc, wc, by = "word", all = TRUE)
}
total_wc$freq
rowMeans(total_wc)
rowMeans(total_wc, na.rm = TRUE)
total_wc
rowMeans(total_wc %>% select(-word), na.rm = TRUE)
total_wc$freq = rowMeans(total_wc %>% select(-word), na.rm = TRUE)
# creat wordcloud
set.seed(1234)
wordcloud(words = total_wc$word, freq = total_wc$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
# creat wordcloud
set.seed(1234)
wordcloud(words = total_wc$word, freq = total_wc$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud2(data=total_wc, size = 0.7, shape = 'pentagon')
total_wc
wordcloud2(data=total_wc %>% select(word,freq), size = 0.7, shape = 'pentagon')
wordcloud2(data=total_wc %>% select(word,freq),
size = 0.7, shape = 'rectangle',
maxRotation = 0.1)
getWC_mat = function(text, w){
stopwords = c("just","usually","sometimes","something",
"also","every")
docs <- Corpus(VectorSource(text))
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, stopwords)
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
# docs <- tm_map(docs, stemDocument)
## Build a term-document matrix
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v*w)
return(d)
}
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
total_wc = getWC_mat(df$OpenF[1], df$fhab[1])
colnames(total_wc)[2] = "col1"
for (i in c(2:dim(df)[1])){
wc = getWC_mat(df$OpenF[i], df$fhab[i])
colnames(wc)[2] = paste0("col",i)
total_wc = merge(total_wc, wc, by = "word", all = TRUE)
}
total_wc$freq = rowMeans(total_wc %>% select(-word), na.rm = TRUE)
# creat wordcloud
set.seed(1234)
wordcloud(words = total_wc$word, freq = total_wc$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
library(wordcloud2)
wordcloud2(data=total_wc %>% select(word,freq),
size = 0.7, shape = 'rectangle',
maxRotation = 0.1)
wordcloud2(data=total_wc %>% select(word,freq),
size = 0.7, shape = 'rectangle',
minRotation = -pi/6, maxRotation = -pi/6, rotateRatio = 1)
wordcloud2(data=total_wc %>% select(word,freq),
size = 0.7, shape = 'rectangle',
minRotation = 0, maxRotation = 0, rotateRatio = 1))
wordcloud2(data=total_wc %>% select(word,freq),
size = 0.7, shape = 'rectangle',
minRotation = 0, maxRotation = 0, rotateRatio = 1)
df$fhab_s = scale(df$fhab)
df$fhab_s = scale(df$fhab)
total_wc = getWC_mat(df$OpenF[1], df$fhab_s[1])
colnames(total_wc)[2] = "col1"
for (i in c(2:dim(df)[1])){
wc = getWC_mat(df$OpenF[i], df$fhab_s[i])
colnames(wc)[2] = paste0("col",i)
total_wc = merge(total_wc, wc, by = "word", all = TRUE)
}
total_wc$freq = rowMeans(total_wc %>% select(-word), na.rm = TRUE)
# creat wordcloud
## FB connectedness
df$fconnected_s = scale(df$fconnected_s)
total_wc2 = getWC_mat(df$OpenF[1], df$fconnected_s[1])
colnames(total_wc2)[2] = "col1"
for (i in c(2:dim(df)[1])){
wc = getWC_mat(df$OpenF[i], df$fconnected_s[i])
colnames(wc)[2] = paste0("col",i)
total_wc2 = merge(total_wc2, wc, by = "word", all = TRUE)
}
total_wc2$freq = rowMeans(total_wc2 %>% select(-word),
na.rm = TRUE)
df$fconnected_s = scale(df$fconnected_s)
total_wc2 = getWC_mat(df$OpenF[1], df$fconnected_s[1])
colnames(total_wc2)[2] = "col1"
getWC_mat(df$OpenF[1], df$fconnected_s[1])
getWC_mat(df$OpenF[1], df$fconnected_s[1])
df$OpenF[1]
df$fconnected_s[1]
df$fconnected_s = scale(df$fconnected_s)
## FB connectedness
df$fconnected_s = scale(df$fconnected)
total_wc2 = getWC_mat(df$OpenF[1], df$fconnected_s[1])
colnames(total_wc2)[2] = "col1"
for (i in c(2:dim(df)[1])){
wc = getWC_mat(df$OpenF[i], df$fconnected_s[i])
colnames(wc)[2] = paste0("col",i)
total_wc2 = merge(total_wc2, wc, by = "word", all = TRUE)
}
total_wc2$freq = rowMeans(total_wc2 %>% select(-word),
na.rm = TRUE)
wordcloud2(data=total_wc2 %>% select(word,freq),
size = 0.7, shape = 'rectangle',
minRotation = 0, maxRotation = 0, rotateRatio = 1)
wordcloud2(data=total_wc %>% select(word,freq),
size = 0.7, shape = 'rectangle',
minRotation = 0, maxRotation = 0, rotateRatio = 1,
hoverFunction = FALSE)
wordcloud2(data=total_wc2 %>% select(word,freq),
size = 0.7, shape = 'rectangle',
minRotation = 0, maxRotation = 0, rotateRatio = 1)
df = read.csv("/Users/Rui/Box Sync/CurrentProjects_Penn/PSAs/05_Analyses/Rui/brain_pe/03_Realcost_datasheets/individualAdLevelData.csv")
df2 = df %>% group_by(pid) %>%
mutate(pe_s = scale(pe0),
share_s = scale(share))
df2
df2$pe_s
View(df2)
df2 = df %>% group_by(pid) %>%
mutate(pe_s = scale(pe0),
share_s = scale(share)) %>%
select(pid, pe0, share)
View(df2)
df = read.csv("/Users/Rui/Box Sync/CurrentProjects_Penn/PSAs/05_Analyses/Rui/brain_pe/03_Realcost_datasheets/individualAdLevelData.csv")
df2 = df %>% group_by(pid) %>%
mutate(pe_s = scale(pe0),
share_s = scale(share)) %>%
select(pid, pe_s, share_s)
View(df2)
sum(df2$pe_s[which(df2$pid == "PSA003")])
scale_this <- function(x){
(x - mean(x, na.rm=TRUE)) / sd(x, na.rm=TRUE)
}
df = read.csv("/Users/Rui/Box Sync/CurrentProjects_Penn/PSAs/05_Analyses/Rui/brain_pe/03_Realcost_datasheets/individualAdLevelData.csv")
scale_this <- function(x){
(x - mean(x, na.rm=TRUE)) / sd(x, na.rm=TRUE)
}
df2 = df %>% group_by(pid) %>%
mutate(pe_s = scale_this(pe0),
share_s = scale_this(share)) %>%
select(pid, pe_s, share_s)
View(df2)
sum(df2$pe_s[which(df2$pid == "PSA003")])
scale_this <- function(x) as.vector(scale(x))
df2 = df %>% group_by(pid) %>%
mutate(pe_s = scale_this(pe0),
share_s = scale_this(share)) %>%
select(pid, pe_s, share_s)
sum(df2$pe_s[which(df2$pid == "PSA003")])
scale_this <- function(x) as.vector(scale(x, center = TRUE))
df2 = df %>% group_by(pid) %>%
mutate(pe_s = scale_this(pe0),
share_s = scale_this(share)) %>%
select(pid, pe_s, share_s)
sum(df2$pe_s[which(df2$pid == "PSA003")])
select(pid, pe_s, share_s)
df2 =
df %>%
group_by(pid) %>%
mutate_all(list(scaled = scale))
df2 =
df %>%
group_by(pid) %>%
mutate_all(list(scaled = scale))
df2 =
df %>%
select(pid, pe0, share) %>%
group_by(pid) %>%
mutate_all(list(scaled = scale))
colnames(df)
df2 =
df %>%
select(pid, pe0, share) %>%
group_by(pid) %>%
mutate_all(list(scaled = scale))
df2
sum(df2$pe0_scaled[which(df2$pid == "PSA003")])
sum(df2$pe0_scaled[which(df2$pid == "PSA005")])
sd(df2$pe0_scaled[which(df2$pid == "PSA005")])
colnames(df2) = c("pid","pe0","share","pe0_s","share_s")
df3 = df2 %>%
group_by(pid) %>%
mutate(pe_ranks = order(order(pe0_s, decreasing=TRUE)),
share_ranks = order(order(share_s, decreasing=TRUE)))
View(df3)
df3 = df2 %>%
group_by(pid) %>%
mutate(pe_ranks = order(pe0_s, decreasing=TRUE),
share_ranks = order(share_s, decreasing=TRUE))
View(df3)
df3 = df2 %>%
group_by(pid) %>%
mutate(pe_ranks = rank(pe0_s, decreasing=TRUE),
share_ranks = rank(share_s, decreasing=TRUE))
df3 = df2 %>%
group_by(pid) %>%
mutate(pe_ranks = rank(pe0_s),
share_ranks = rank(share_s))
View(df3)
detach("package:plyr", unload=TRUE)
df3 = df2 %>%
group_by(pid) %>%
mutate(pe_ranks = rank(pe0_s),
share_ranks = rank(share_s))
View(df3)
df3 = df2 %>%
group_by(pid) %>%
mutate(pe_ranks = order(pe0_s, decreasing = T),
share_ranks = order(share_s, decreasing = T))
View(df3)
df3 = df2 %>%
group_by(pid) %>%
mutate(pe_ranks = order(order(pe0_s, decreasing = T)),
share_ranks = order(order(share_s, decreasing = T)))
View(df3)
df2 =
df %>%
select(pid, vid,pe0, share) %>%
group_by(pid) %>%
mutate_at(c("pe0","share"),list(scaled = scale))
colnames(df2) = c("pid","vid","pe0","share","pe0_s","share_s")
df3 = df2 %>%
group_by(pid) %>%
mutate(pe_ranks = order(order(pe0_s, decreasing = T)),
share_ranks = order(order(share_s, decreasing = T)))
View(df3)
df3$pe_top3 = NA
df3$share_top3 = NA
df3$pe_top3[which(df3$pe_rank < 4)] = 1
df3$pe_top3[which(df3$pe_rank > 9)] = 0
df3$share_top3[which(df3$share_rank < 4)] = 1
df3$share_top3[which(df3$share_rank > 9)] = 0
df3 = df2 %>%
group_by(pid) %>%
mutate(pe_rank = order(order(pe0_s, decreasing = T)),
share_rank = order(order(share_s, decreasing = T)))
df3$pe_top3 = NA
df3$share_top3 = NA
df3$pe_top3[which(df3$pe_rank < 4)] = 1
df3$pe_top3[which(df3$pe_rank > 9)] = 0
df3$share_top3[which(df3$share_rank < 4)] = 1
df3$share_top3[which(df3$share_rank > 9)] = 0
write.csv("/Users/Rui/Desktop/realcost_data.csv")
write.csv(df3,"/Users/Rui/Desktop/realcost_data.csv",
row.names = FALSE)
rm(list = ls())
if (!require("pacman")) install.packages("pacman",repos = "http://cran.us.r-project.org")
pacman::p_load(ggplot2,ggpubr,tidyverse,plyr, dplyr, knitr, kableExtra, jtools,pander)
source("~/Documents/GitHub/facebookUse/functions.R")
pd <- position_dodge(0.3) # move them .05 to the left and right
path =  "~/Box Sync/CurrentProjects_Penn/TPS12_BART/99_fomo/github/"
setwd(path)
path =  "~/Box Sync/CurrentProjects_Penn/TPS12_BART/99_fomo/github (peirui@upenn.edu)/"
setwd(path)
## socialMedia data
socialMedia = read.csv(paste0(path, '0_facebook/tps2_facebook.csv'),na.strings=c("","NA"), stringsAsFactors = FALSE)[-1,]
socialMedia = socialMedia %>%
mutate(TPS_ID = as.factor(TPS_ID),
OpenF = as.factor(OpenF)) %>%
mutate_if(is.character,as.numeric) %>%
mutate(fhab = rowMeans(.[,10:19], na.rm = FALSE),
femo = rowMeans(.[c(8,9,20:39)], na.rm = FALSE),
fconnected = rowMeans(.[c(8,9)], na.rm = FALSE),
femopost = rowMeans(.[c(20:39)], na.rm = FALSE),
TPS_ID = as.character(TPS_ID),
OpenF = as.character(OpenF)) %>%
select(TPS_ID, fhab, femo,fconnected,femopost, OpenF)
cyber = read.csv(paste0(path, '0_cyberball/cyberball_roi.csv'), stringsAsFactors = FALSE)
age = read.csv(paste0(path, '0_surveys/tps12_scanner_study_age.csv'), stringsAsFactors = FALSE) %>%
select(TPS_ID, age, study_t, scanner_t)
