<<<<<<< Updated upstream
=======
md3.4 = lm(precuneus_roi_mask ~ fconnected +study_t + scanner_t + age, data = df)
sum3.4 = summ(md3.4, vifs = TRUE,confint = TRUE, ci.width = 0.95, digits = 3)
md3.5 = lm(lTpj_roi_mask ~ fconnected +study_t + scanner_t + age, data = df)
sum3.5 = summ(md3.5, vifs = TRUE,confint = TRUE, ci.width = 0.95, digits = 3)
md3.6 = lm(rTpj_roi_mask ~ fconnected +study_t + scanner_t + age, data = df)
sum3.6 = summ(md3.6, vifs = TRUE,confint = TRUE, ci.width = 0.95, digits = 3)
md3.7 = lm(rSts_roi_mask ~ fconnected +study_t + scanner_t + age, data = df)
sum3.7 = summ(md3.7, vifs = TRUE,confint = TRUE, ci.width = 0.95, digits = 3)
export_summs(md3.1, md3.2, md3.3, md3.4, md3.5,md3.6, md3.7,scale = FALSE,
error_format = "[{conf.low}, {conf.high}]",
to.file = "xlsx", file.name = "table2.xlsx")
md5.1 = lm(ado_vs_mask ~ fhab +study_t +scanner_t + age, data = df)
md5.2 = lm(ado_lateralOFC_mask ~ fhab +study_t +scanner_t + age, data = df)
export_summs(md5.1, md5.2,scale = FALSE,
error_format = "[{conf.low}, {conf.high}]",
to.file = "xlsx", file.name = "table3.xlsx")
md3.1 = lm(dmpfc_roi_mask ~ fhab +study_t + scanner_t + age, data = df)
md3.1 = lm(cacioppo_dacc_mask ~ fconnected +study_t + scanner_t + age, data = df)
summ(md3.1)
rm(list = ls())
if (!require("pacman")) install.packages("pacman",repos = "http://cran.us.r-project.org")
pacman::p_load(ggplot2,ggpubr,tidyverse,plyr, dplyr, knitr, kableExtra, jtools,pander)
source("~/Documents/GitHub/facebookUse/functions.R")
pd <- position_dodge(0.3) # move them .05 to the left and right
path =  "/Users/Rui/Box Sync/CurrentProjects_Penn/TPS12_BART/99_fomo/github (peirui@upenn.edu)/"
setwd(path)
## socialMedia data
socialMedia = read.csv(paste0(path, '0_facebook/tps2_facebook.csv'),na.strings=c("","NA"), stringsAsFactors = FALSE)[-1,]
socialMedia = socialMedia %>%
mutate(TPS_ID = as.factor(TPS_ID),
OpenF = as.factor(OpenF)) %>%
mutate_if(is.character,as.numeric) %>%
mutate(fhab = rowMeans(.[,10:19], na.rm = FALSE),
femo = rowMeans(.[c(8,9,20:39)], na.rm = FALSE),
fconnected = rowMeans(.[c(8,9)], na.rm = FALSE),
femopost = rowMeans(.[c(20:39)], na.rm = FALSE),
TPS_ID = as.character(TPS_ID),
OpenF = as.character(OpenF)) %>%
select(TPS_ID, fhab, femo,fconnected,femopost,ConnFri_2,ConnFam_2, OpenF)
cyber = read.csv(paste0(path, '0_cyberball/cyberball_roi.csv'), stringsAsFactors = FALSE)
age = read.csv(paste0(path, '0_surveys/demo.csv'), stringsAsFactors = FALSE) %>%
select(TPS_ID, age, study_t, scanner_t)
iri = read.csv(paste0(path, '0_surveys/iri_tps12.csv'), stringsAsFactors = FALSE)
iri = iri %>%
mutate(IRI_mean = rowMeans(select(iri, starts_with("IRI_")), na.rm = FALSE))
nts = read.csv(paste0(path, '0_surveys/nts.csv'), stringsAsFactors = FALSE)
df = cyber %>%
left_join(socialMedia, by = "TPS_ID") %>%
left_join(age, by = "TPS_ID") %>%
left_join(iri, by = "TPS_ID") %>%
left_join(nts, by = "TPS_ID")
# Remove NAs
df = df %>% drop_na(fhab,femo, NTS_Belongingness,
NTS_SelfEsteem, NTS_Control,
NTS_MeaningfulExistence, study_t, scanner_t,
IRI_Perspective_Taking, IRI_Fantasy,
IRI_Empathic_Concern, IRI_Personal_Distress,
fconnected) %>%
mutate(NTS_Belongingness = 7 - NTS_Belongingness,
NTS_SelfEsteem = 7 - NTS_SelfEsteem,
NTS_Control = 7 - NTS_Control,
NTS_MeaningfulExistence = 7 - NTS_MeaningfulExistence,
NTS_Mean = 7 - NTS_Mean)
cor.test(df$ConnFri_2, df$ConnFam_2)
md = lm(ConnFri_2 ~ ConnFam_2 + study_t + age, data =df )
summ(md)
md = lm(ConnFri_2 ~ ConnFam_2  + age, data =df)
summ(md)
md = lm(ConnFri_2 ~ ConnFam_2  + age, data =df)
summ(md)
ggscatter(df, "ConnFam_2", "ConnFri_2")
cor.test(df$ConnFam_2, df$ConnFri_2, method = "spearman")
cor.test(df$ConnFam_2, df$ConnFri_2, method = "kendall")
library(robust)
md = rlm(ConnFri_2 ~ ConnFam_2  + age, data =df)
detach("package:robust", unload = TRUE)
ggscatter(df, "ConnFam_2", "ConnFri_2", repel = TRUE)
ggscatter(df, "ConnFam_2", "ConnFri_2", repel = TRUE,add = "reg.line")
md = rlm(ConnFri_2 ~ ConnFam_2  + fhab + age, data =df)
md = lm(ConnFri_2 ~ ConnFam_2  + fhab + age, data =df)
summ(md)
md = lm(ConnFri_2 ~ ConnFam_2  + age + study_t + scanner_t, data =df)
summ(md)
summ(md, vifs = TRUE)
psych::alpha(df %>% select(ConnFam_2,ConnFri_2))
df %>% select(ConnFam_2,ConnFri_2)
df$ConnFam_2 = scale(df$ConnFam_2)
df$ConnFri_2 = scale(df$ConnFri_2)
cor.test(df$ConnFam_2, df$ConnFri_2, method = "kendall")
rm(list = ls())
if (!require("pacman")) install.packages("pacman",repos = "http://cran.us.r-project.org")
pacman::p_load(ggplot2,ggpubr,tidyverse,plyr, dplyr, knitr, kableExtra, jtools,pander)
source("~/Documents/GitHub/facebookUse/functions.R")
pd <- position_dodge(0.3) # move them .05 to the left and right
path =  "/Users/Rui/Box Sync/CurrentProjects_Penn/TPS12_BART/99_fomo/github (peirui@upenn.edu)/"
setwd(path)
## socialMedia data
socialMedia = read.csv(paste0(path, '0_facebook/tps2_facebook.csv'),na.strings=c("","NA"), stringsAsFactors = FALSE)[-1,]
socialMedia = socialMedia %>%
mutate(TPS_ID = as.factor(TPS_ID),
OpenF = as.factor(OpenF)) %>%
mutate_if(is.character,as.numeric) %>%
mutate(fhab = rowMeans(.[,10:19], na.rm = FALSE),
femo = rowMeans(.[c(8,9,20:39)], na.rm = FALSE),
fconnected = rowMeans(.[c(8,9)], na.rm = FALSE),
femopost = rowMeans(.[c(20:39)], na.rm = FALSE),
TPS_ID = as.character(TPS_ID),
OpenF = as.character(OpenF)) %>%
select(TPS_ID, fhab, femo,fconnected,femopost,ConnFri_2,ConnFam_2, OpenF)
cyber = read.csv(paste0(path, '0_cyberball/cyberball_roi.csv'), stringsAsFactors = FALSE)
age = read.csv(paste0(path, '0_surveys/demo.csv'), stringsAsFactors = FALSE) %>%
select(TPS_ID, age, study_t, scanner_t)
iri = read.csv(paste0(path, '0_surveys/iri_tps12.csv'), stringsAsFactors = FALSE)
iri = iri %>%
mutate(IRI_mean = rowMeans(select(iri, starts_with("IRI_")), na.rm = FALSE))
nts = read.csv(paste0(path, '0_surveys/nts.csv'), stringsAsFactors = FALSE)
df = cyber %>%
left_join(socialMedia, by = "TPS_ID") %>%
left_join(age, by = "TPS_ID") %>%
left_join(iri, by = "TPS_ID") %>%
left_join(nts, by = "TPS_ID")
# Remove NAs
df = df %>% drop_na(fhab,femo, NTS_Belongingness,
NTS_SelfEsteem, NTS_Control,
NTS_MeaningfulExistence, study_t, scanner_t,
IRI_Perspective_Taking, IRI_Fantasy,
IRI_Empathic_Concern, IRI_Personal_Distress,
fconnected) %>%
mutate(NTS_Belongingness = 7 - NTS_Belongingness,
NTS_SelfEsteem = 7 - NTS_SelfEsteem,
NTS_Control = 7 - NTS_Control,
NTS_MeaningfulExistence = 7 - NTS_MeaningfulExistence,
NTS_Mean = 7 - NTS_Mean)
tps1 = df %>% filter( study_t == "tps1")
tps2 = df %>% filter( study_t == "tps2")
round(mean(df$age),2)
round(sd(df$age),2)
round(mean(tps1$age),2)
round(sd(tps1$age),2)
round(mean(tps2$age),2)
round(sd(tps2$age),2)
t.test(tps1$age, tps2$age)
df %>% select(ConnFam_2,ConnFri_2)
md = lm(ado_rois_mask~ ConnFri_2+ scanner_t + study_t + age , data = df)
summ(md, confint = T, scale = F)
md = lm(ado_vs_mask~ ConnFri_2+ scanner_t + study_t + age, data = df)
summ(md, confint = T, scale = F)
md = lm(ado_lateralOFC_mask~ ConnFri_2+ scanner_t + study_t + age, data = df)
summ(md, confint = T, scale = F)
md = lm(saxe_combined_mask~ ConnFri_2+ scanner_t + study_t + age, data = df)
summ(md, confint = T, scale = F)
md = lm(ado_rois_mask~ ConnFam_2+ scanner_t + study_t + age , data = df)
summ(md, confint = T, scale = F)
md = lm(ado_vs_mask~ ConnFam_2+ scanner_t + study_t + age, data = df)
summ(md, confint = T, scale = F)
md = lm(ado_lateralOFC_mask~ ConnFam_2+ scanner_t + study_t + age, data = df)
summ(md, confint = T, scale = F)
md = lm(saxe_combined_mask~ ConnFam_2+ scanner_t + study_t + age, data = df)
summ(md, confint = T, scale = F)
md = lm(NTS_Mean~ ConnFri_2+ scanner_t + study_t + age, data = df)
summ(md, confint = T, scale = F)
md = lm(NTS_Mean~ ConnFam_2+ scanner_t + study_t + age, data = df)
summ(md, confint = T, scale = F)
ggplot(df, aes(ConnFam_2, ConnFri_2)) +
geom_point(position=position_jitter(h=0.1, w=0.1),
shape = 21, alpha = 0.5, size = 3)
tps1$ConnFam_2_s = scale(tps1$ConnFam_2)
tps1$ConnFri_2_s = scale(tps1$ConnFri_2)
tps2$ConnFam_2_s = scale(tps2$ConnFam_2)
tps2$ConnFri_2_s = scale(tps2$ConnFri_2)
df = rbind(tps1, tps2)
md = lm(ConnFri_2 ~ ConnFam_2  + age + study_t, data =df)
summ(md, vifs = TRUE)
md = lm(ConnFri_2_s~ ConnFam_2_s  + age + study_t, data =df)
summ(md, vifs = TRUE)
ggscatter(df, "ConnFam_2", "ConnFri_2", repel = TRUE,add = "reg.line")
ggscatter(df, "ConnFam_2_s", "ConnFri_2_s", repel = TRUE,add = "reg.line")
ggplot(df, aes(ConnFam_2_s, ConnFri_2_s)) +
geom_point(position=position_jitter(h=0.1, w=0.1),
shape = 21, alpha = 0.5, size = 3)
which(df$ConnFam_2_s>2)
df$ConnFam_2_s[13]
df$ConnFam_2_s[30]
df$ConnFam_2_s[41]
df = df[-13.]
df = rbind(tps1, tps2)
df = df[-13,]
cor.test(df$ConnFam_2, df$ConnFri_2, method = "kendall")
cor.test(df$ConnFam_2, df$ConnFri_2, method = "pearson")
cor.test(df$ConnFam_2_s, df$ConnFam_2_s, method = "pearson")
cor.test(df$ConnFam_2_s, df$ConnFri_2_s, method = "pearson")
cor.test(df$ConnFam_2, df$ConnFri_2, method = "pearson")
cor.test(df$ConnFam_2, df$ConnFri_2, method = "spearman")
cor.test(socialMedia$ConnFam_2, socialMedia$ConnFri_2, method = "spearman")
cor.test(socialMedia$ConnFam_2, socialMedia$ConnFri_2, method = "pearson")
socialMedia %>% drop_na(fhab, femo)
socialMedia2 = socialMedia %>% drop_na(fhab, femo)
cor.test(socialMedia2$ConnFam_2, socialMedia2$ConnFri_2, method = "pearson")
intersect(socialMedia2$TPS_ID, df$TPS_ID)
socialMedia2 = socialMedia(which(socialMedia$TPS_ID %in% df$TPS_ID))
socialMedia2 = socialMedia[which(socialMedia$TPS_ID %in% df$TPS_ID),]
rm(list = ls())
if (!require("pacman")) install.packages("pacman",repos = "http://cran.us.r-project.org")
pacman::p_load(ggplot2,ggpubr,tidyverse,plyr, dplyr, knitr, kableExtra, jtools,pander)
source("~/Documents/GitHub/facebookUse/functions.R")
pd <- position_dodge(0.3) # move them .05 to the left and right
path =  "/Users/Rui/Box Sync/CurrentProjects_Penn/TPS12_BART/99_fomo/github (peirui@upenn.edu)/"
setwd(path)
## socialMedia data
socialMedia = read.csv(paste0(path, '0_facebook/tps2_facebook.csv'),na.strings=c("","NA"), stringsAsFactors = FALSE)[-1,]
socialMedia = socialMedia %>%
mutate(TPS_ID = as.factor(TPS_ID),
OpenF = as.factor(OpenF)) %>%
mutate_if(is.character,as.numeric) %>%
mutate(fhab = rowMeans(.[,10:19], na.rm = FALSE),
femo = rowMeans(.[c(8,9,20:39)], na.rm = FALSE),
fconnected = rowMeans(.[c(8,9)], na.rm = FALSE),
femopost = rowMeans(.[c(20:39)], na.rm = FALSE),
TPS_ID = as.character(TPS_ID),
OpenF = as.character(OpenF)) %>%
select(TPS_ID, fhab, femo,fconnected,femopost,ConnFri_2,ConnFam_2, OpenF)
cyber = read.csv(paste0(path, '0_cyberball/cyberball_roi.csv'), stringsAsFactors = FALSE)
age = read.csv(paste0(path, '0_surveys/demo.csv'), stringsAsFactors = FALSE) %>%
select(TPS_ID, age, study_t, scanner_t)
iri = read.csv(paste0(path, '0_surveys/iri_tps12.csv'), stringsAsFactors = FALSE)
iri = iri %>%
mutate(IRI_mean = rowMeans(select(iri, starts_with("IRI_")), na.rm = FALSE))
nts = read.csv(paste0(path, '0_surveys/nts.csv'), stringsAsFactors = FALSE)
df = cyber %>%
left_join(socialMedia, by = "TPS_ID") %>%
left_join(age, by = "TPS_ID") %>%
left_join(iri, by = "TPS_ID") %>%
left_join(nts, by = "TPS_ID")
# Remove NAs
df = df %>% drop_na(fhab,femo, NTS_Belongingness,
NTS_SelfEsteem, NTS_Control,
NTS_MeaningfulExistence, study_t, scanner_t,
IRI_Perspective_Taking, IRI_Fantasy,
IRI_Empathic_Concern, IRI_Personal_Distress,
fconnected) %>%
mutate(NTS_Belongingness = 7 - NTS_Belongingness,
NTS_SelfEsteem = 7 - NTS_SelfEsteem,
NTS_Control = 7 - NTS_Control,
NTS_MeaningfulExistence = 7 - NTS_MeaningfulExistence,
NTS_Mean = 7 - NTS_Mean)
socialMedia = socialMedia %>%
mutate(TPS_ID = as.factor(TPS_ID),
OpenF = as.factor(OpenF)) %>%
mutate_if(is.character,as.numeric) %>%
mutate(fhab = rowMeans(.[,10:19], na.rm = FALSE),
femo = rowMeans(.[c(8,9,20:39)], na.rm = FALSE),
fconnected = rowMeans(.[c(8,9)], na.rm = FALSE),
femopost = rowMeans(.[c(20:39)], na.rm = FALSE),
TPS_ID = as.character(TPS_ID),
OpenF = as.character(OpenF)) %>%
drop_na(femo, fhab) %>%
select(TPS_ID, fhab, femo,fconnected,femopost,ConnFri_2,ConnFam_2, OpenF)
socialMedia = read.csv(paste0(path, '0_facebook/tps2_facebook.csv'),na.strings=c("","NA"), stringsAsFactors = FALSE)[-1,]
socialMedia = socialMedia %>%
mutate(TPS_ID = as.factor(TPS_ID),
OpenF = as.factor(OpenF)) %>%
mutate_if(is.character,as.numeric) %>%
mutate(fhab = rowMeans(.[,10:19], na.rm = FALSE),
femo = rowMeans(.[c(8,9,20:39)], na.rm = FALSE),
fconnected = rowMeans(.[c(8,9)], na.rm = FALSE),
femopost = rowMeans(.[c(20:39)], na.rm = FALSE),
TPS_ID = as.character(TPS_ID),
OpenF = as.character(OpenF)) %>%
drop_na(femo, fhab) %>%
select(TPS_ID, fhab, femo,fconnected,femopost,ConnFri_2,ConnFam_2, OpenF)
path =  "/Users/Rui/Box Sync/CurrentProjects_Penn/TPS12_BART/99_fomo/github (peirui@upenn.edu)/"
setwd(path)
## socialMedia data
socialMedia = read.csv(paste0(path, '0_facebook/tps2_facebook.csv'),na.strings=c("","NA"), stringsAsFactors = FALSE)[-1,]
socialMedia = socialMedia %>%
mutate(TPS_ID = as.factor(TPS_ID),
OpenF = as.factor(OpenF)) %>%
mutate_if(is.character,as.numeric) %>%
mutate(fhab = rowMeans(.[,10:19], na.rm = FALSE),
femo = rowMeans(.[c(8,9,20:39)], na.rm = FALSE),
fconnected = rowMeans(.[c(8,9)], na.rm = FALSE),
femopost = rowMeans(.[c(20:39)], na.rm = FALSE),
TPS_ID = as.character(TPS_ID),
OpenF = as.character(OpenF)) %>%
drop_na(femo, fhab) %>%
select(TPS_ID, fhab, femo,fconnected,femopost,ConnFri_2,ConnFam_2, OpenF)
cyber = read.csv(paste0(path, '0_cyberball/cyberball_roi.csv'), stringsAsFactors = FALSE)
age = read.csv(paste0(path, '0_surveys/demo.csv'), stringsAsFactors = FALSE) %>%
select(TPS_ID, age, study_t, scanner_t)
iri = read.csv(paste0(path, '0_surveys/iri_tps12.csv'), stringsAsFactors = FALSE)
iri = iri %>%
mutate(IRI_mean = rowMeans(select(iri, starts_with("IRI_")), na.rm = FALSE))
nts = read.csv(paste0(path, '0_surveys/nts.csv'), stringsAsFactors = FALSE)
df = cyber %>%
left_join(socialMedia, by = "TPS_ID") %>%
left_join(age, by = "TPS_ID") %>%
left_join(iri, by = "TPS_ID") %>%
left_join(nts, by = "TPS_ID")
# Remove NAs
df = df %>% drop_na(fhab,femo, NTS_Belongingness,
NTS_SelfEsteem, NTS_Control,
NTS_MeaningfulExistence, study_t, scanner_t,
IRI_Perspective_Taking, IRI_Fantasy,
IRI_Empathic_Concern, IRI_Personal_Distress,
fconnected) %>%
mutate(NTS_Belongingness = 7 - NTS_Belongingness,
NTS_SelfEsteem = 7 - NTS_SelfEsteem,
NTS_Control = 7 - NTS_Control,
NTS_MeaningfulExistence = 7 - NTS_MeaningfulExistence,
NTS_Mean = 7 - NTS_Mean)
85 - 66
cor.test(socialMedia$ConnFam_2, socialMedia$ConnFri_2, method = "pearson")
socialMedia = read.csv(paste0(path, '0_facebook/tps2_facebook.csv'),na.strings=c("","NA"), stringsAsFactors = FALSE)[-1,]
colnames(socialMedia)
path =  "/Users/Rui/Box Sync/CurrentProjects_Penn/TPS12_BART/99_fomo/github (peirui@upenn.edu)/"
setwd(path)
>>>>>>> Stashed changes
## socialMedia data
socialMedia = read.csv(paste0(path, '0_facebook/tps2_facebook.csv'),na.strings=c("","NA"), stringsAsFactors = FALSE)[-1,]
socialMedia = socialMedia %>%
mutate(TPS_ID = as.factor(TPS_ID),
OpenF = as.factor(OpenF)) %>%
mutate_if(is.character,as.numeric) %>%
mutate(fhab = rowMeans(.[,10:19], na.rm = FALSE),
femo = rowMeans(.[c(8,9,20:39)], na.rm = FALSE),
fconnected = rowMeans(.[c(8,9)], na.rm = FALSE),
femopost = rowMeans(.[c(20:39)], na.rm = FALSE),
TPS_ID = as.character(TPS_ID),
OpenF = as.character(OpenF)) %>%
drop_na(femo, fhab) %>%
select(TPS_ID, fhab, femo,fconnected,femopost,ConnFri_2,ConnFam_2, OpenF,
HabF_1, HabF_2, HabF_3, HabF_4, HabF_5, HabF_6, HabF_7, HabF_8, HabF_9,HabF_10)
cyber = read.csv(paste0(path, '0_cyberball/cyberball_roi.csv'), stringsAsFactors = FALSE)
age = read.csv(paste0(path, '0_surveys/demo.csv'), stringsAsFactors = FALSE) %>%
select(TPS_ID, age, study_t, scanner_t)
iri = read.csv(paste0(path, '0_surveys/iri_tps12.csv'), stringsAsFactors = FALSE)
iri = iri %>%
mutate(IRI_mean = rowMeans(select(iri, starts_with("IRI_")), na.rm = FALSE))
nts = read.csv(paste0(path, '0_surveys/nts.csv'), stringsAsFactors = FALSE)
df = cyber %>%
left_join(socialMedia, by = "TPS_ID") %>%
left_join(age, by = "TPS_ID") %>%
left_join(iri, by = "TPS_ID") %>%
left_join(nts, by = "TPS_ID")
# Remove NAs
df = df %>% drop_na(fhab,femo, NTS_Belongingness,
NTS_SelfEsteem, NTS_Control,
NTS_MeaningfulExistence, study_t, scanner_t,
IRI_Perspective_Taking, IRI_Fantasy,
IRI_Empathic_Concern, IRI_Personal_Distress,
fconnected) %>%
mutate(NTS_Belongingness = 7 - NTS_Belongingness,
NTS_SelfEsteem = 7 - NTS_SelfEsteem,
NTS_Control = 7 - NTS_Control,
NTS_MeaningfulExistence = 7 - NTS_MeaningfulExistence,
NTS_Mean = 7 - NTS_Mean)
psych::alpha(df %>% select(HabF_1, HabF_2, HabF_3, HabF_4, HabF_5, HabF_6, HabF_7, HabF_8, HabF_9,HabF_10))
psych::alpha(socialMedia %>% select(HabF_1, HabF_2, HabF_3, HabF_4, HabF_5, HabF_6, HabF_7, HabF_8, HabF_9,HabF_10))
psych::alpha(socialMedia %>% select(ConnFam_2, ConnFri_2))
rm(list = ls())
if (!require("pacman")) install.packages("pacman",repos = "http://cran.us.r-project.org")
pacman::p_load(ggplot2,ggpubr,tidyverse,plyr, dplyr, knitr, kableExtra, jtools,pander)
source("~/Documents/GitHub/facebookUse/functions.R")
pd <- position_dodge(0.3) # move them .05 to the left and right
path =  "/Users/Rui/Box Sync/CurrentProjects_Penn/TPS12_BART/99_fomo/github (peirui@upenn.edu)/"
setwd(path)
## socialMedia data
socialMedia = read.csv(paste0(path, '0_facebook/tps2_facebook.csv'),na.strings=c("","NA"), stringsAsFactors = FALSE)[-1,]
socialMedia = socialMedia %>%
mutate(TPS_ID = as.factor(TPS_ID),
OpenF = as.factor(OpenF)) %>%
mutate_if(is.character,as.numeric) %>%
mutate(fhab = rowMeans(.[,10:19], na.rm = FALSE),
femo = rowMeans(.[c(8,9,20:39)], na.rm = FALSE),
fconnected = rowMeans(.[c(8,9)], na.rm = FALSE),
femopost = rowMeans(.[c(20:39)], na.rm = FALSE),
TPS_ID = as.character(TPS_ID),
OpenF = as.character(OpenF)) %>%
drop_na(femo, fhab) %>%
select(TPS_ID, fhab, femo,fconnected,femopost,ConnFri_2,ConnFam_2, OpenF,
HabF_1, HabF_2, HabF_3, HabF_4, HabF_5, HabF_6, HabF_7, HabF_8, HabF_9,HabF_10)
cyber = read.csv(paste0(path, '0_cyberball/cyberball_roi.csv'), stringsAsFactors = FALSE)
age = read.csv(paste0(path, '0_surveys/demo.csv'), stringsAsFactors = FALSE) %>%
select(TPS_ID, age, study_t, scanner_t)
iri = read.csv(paste0(path, '0_surveys/iri_tps12.csv'), stringsAsFactors = FALSE)
iri = iri %>%
mutate(IRI_mean = rowMeans(select(iri, starts_with("IRI_")), na.rm = FALSE))
nts = read.csv(paste0(path, '0_surveys/nts.csv'), stringsAsFactors = FALSE)
df = cyber %>%
left_join(socialMedia, by = "TPS_ID") %>%
left_join(age, by = "TPS_ID") %>%
left_join(iri, by = "TPS_ID") %>%
left_join(nts, by = "TPS_ID")
# Remove NAs
df = df %>% drop_na(fhab,femo, NTS_Belongingness,
NTS_SelfEsteem, NTS_Control,
NTS_MeaningfulExistence, study_t, scanner_t,
IRI_Perspective_Taking, IRI_Fantasy,
IRI_Empathic_Concern, IRI_Personal_Distress,
fconnected) %>%
mutate(NTS_Belongingness = 7 - NTS_Belongingness,
NTS_SelfEsteem = 7 - NTS_SelfEsteem,
NTS_Control = 7 - NTS_Control,
NTS_MeaningfulExistence = 7 - NTS_MeaningfulExistence,
NTS_Mean = 7 - NTS_Mean)
<<<<<<< Updated upstream
df[2,"OpenF"]
getWC_mat = function(text, w){
docs <- Corpus(VectorSource(text))
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
# docs <- tm_map(docs, stemDocument)
## Build a term-document matrix
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v*w)
}
getWC_mat(df[2,"OpenF"],3)
getWC_mat = function(text, w){
docs <- Corpus(VectorSource(text))
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
# docs <- tm_map(docs, stemDocument)
## Build a term-document matrix
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v*w)
return(d)
}
getWC_mat(df[2,"OpenF"],3)
getWC_mat(df$OpenF[2], df$fhab[2])
getWC_mat = function(text, w){
docs <- Corpus(VectorSource(text))
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
# docs <- tm_map(docs, stemDocument)
## Build a term-document matrix
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v*w)
return(d)
}
total_wc = getWC_mat(df$OpenF[1], df$fhab[1])
total_wc = getWC_mat(df$OpenF[1], df$fhab[1])
for (i in c(2:dim(df)[1])){
total_wc = merge(total_wc, getWC_mat(df$OpenF[2], df$fhab[2]), by = "word")
}
warnings()
total_wc = getWC_mat(df$OpenF[1], df$fhab[1])
for (i in c(2:dim(df)[1])){
wc = getWC_mat(df$OpenF[2], df$fhab[2])
colnames(wc)[2] = paste0("col",i)
total_wc = merge(total_wc, , by = "word")
}
total_wc = getWC_mat(df$OpenF[1], df$fhab[1])
for (i in c(2:dim(df)[1])){
wc = getWC_mat(df$OpenF[2], df$fhab[2])
colnames(wc)[2] = paste0("col",i)
total_wc = merge(total_wc, wc, by = "word")
}
total_wc
for (i in c(2:dim(df)[1])){
wc = getWC_mat(df$OpenF[i], df$fhab[i])
colnames(wc)[2] = paste0("col",i)
total_wc = merge(total_wc, wc, by = "word")
}
total_wc = getWC_mat(df$OpenF[1], df$fhab[1])
for (i in c(2:dim(df)[1])){
wc = getWC_mat(df$OpenF[i], df$fhab[i])
colnames(wc)[2] = paste0("col",i)
total_wc = merge(total_wc, wc, by = "word")
}
total_wc
colnames(wc)
total_wc = getWC_mat(df$OpenF[1], df$fhab[1])
for (i in c(2:dim(df)[1])){
wc = getWC_mat(df$OpenF[i], df$fhab[i])
colnames(wc)[2] = paste0("col",i)
total_wc = merge(total_wc, wc, by = "word", all = TRUE)
}
total_wc
dim(total_wc)
colnames(wc)[2] = "col1"
total_wc = getWC_mat(df$OpenF[1], df$fhab[1])
colnames(wc)[2] = "col1"
for (i in c(2:dim(df)[1])){
wc = getWC_mat(df$OpenF[i], df$fhab[i])
colnames(wc)[2] = paste0("col",i)
total_wc = merge(total_wc, wc, by = "word", all = TRUE)
}
total_wc$freq
total_wc = getWC_mat(df$OpenF[1], df$fhab[1])
colnames(wc)[2] = "col1"
for (i in c(2:dim(df)[1])){
wc = getWC_mat(df$OpenF[i], df$fhab[i])
colnames(wc)[2] = paste0("col",i)
total_wc = merge(total_wc, wc, by = "word", all = TRUE)
}
total_wc$freq
total_wc = getWC_mat(df$OpenF[1], df$fhab[1])
colnames(wc)[2] = "col1"
total_wc = getWC_mat(df$OpenF[1], df$fhab[1])
colnames(total_wc)[2] = "col1"
for (i in c(2:dim(df)[1])){
wc = getWC_mat(df$OpenF[i], df$fhab[i])
colnames(wc)[2] = paste0("col",i)
total_wc = merge(total_wc, wc, by = "word", all = TRUE)
}
total_wc$freq
rowMeans(total_wc)
rowMeans(total_wc, na.rm = TRUE)
total_wc
rowMeans(total_wc %>% select(-word), na.rm = TRUE)
total_wc$freq = rowMeans(total_wc %>% select(-word), na.rm = TRUE)
# creat wordcloud
set.seed(1234)
wordcloud(words = total_wc$word, freq = total_wc$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
# creat wordcloud
set.seed(1234)
wordcloud(words = total_wc$word, freq = total_wc$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud2(data=total_wc, size = 0.7, shape = 'pentagon')
total_wc
wordcloud2(data=total_wc %>% select(word,freq), size = 0.7, shape = 'pentagon')
wordcloud2(data=total_wc %>% select(word,freq),
size = 0.7, shape = 'rectangle',
maxRotation = 0.1)
getWC_mat = function(text, w){
stopwords = c("just","usually","sometimes","something",
"also","every")
docs <- Corpus(VectorSource(text))
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, stopwords)
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
# docs <- tm_map(docs, stemDocument)
## Build a term-document matrix
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v*w)
return(d)
}
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
total_wc = getWC_mat(df$OpenF[1], df$fhab[1])
colnames(total_wc)[2] = "col1"
for (i in c(2:dim(df)[1])){
wc = getWC_mat(df$OpenF[i], df$fhab[i])
colnames(wc)[2] = paste0("col",i)
total_wc = merge(total_wc, wc, by = "word", all = TRUE)
}
total_wc$freq = rowMeans(total_wc %>% select(-word), na.rm = TRUE)
# creat wordcloud
set.seed(1234)
wordcloud(words = total_wc$word, freq = total_wc$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
library(wordcloud2)
wordcloud2(data=total_wc %>% select(word,freq),
size = 0.7, shape = 'rectangle',
maxRotation = 0.1)
wordcloud2(data=total_wc %>% select(word,freq),
size = 0.7, shape = 'rectangle',
minRotation = -pi/6, maxRotation = -pi/6, rotateRatio = 1)
wordcloud2(data=total_wc %>% select(word,freq),
size = 0.7, shape = 'rectangle',
minRotation = 0, maxRotation = 0, rotateRatio = 1))
wordcloud2(data=total_wc %>% select(word,freq),
size = 0.7, shape = 'rectangle',
minRotation = 0, maxRotation = 0, rotateRatio = 1)
df$fhab_s = scale(df$fhab)
df$fhab_s = scale(df$fhab)
total_wc = getWC_mat(df$OpenF[1], df$fhab_s[1])
colnames(total_wc)[2] = "col1"
for (i in c(2:dim(df)[1])){
wc = getWC_mat(df$OpenF[i], df$fhab_s[i])
colnames(wc)[2] = paste0("col",i)
total_wc = merge(total_wc, wc, by = "word", all = TRUE)
}
total_wc$freq = rowMeans(total_wc %>% select(-word), na.rm = TRUE)
# creat wordcloud
## FB connectedness
df$fconnected_s = scale(df$fconnected_s)
total_wc2 = getWC_mat(df$OpenF[1], df$fconnected_s[1])
colnames(total_wc2)[2] = "col1"
for (i in c(2:dim(df)[1])){
wc = getWC_mat(df$OpenF[i], df$fconnected_s[i])
colnames(wc)[2] = paste0("col",i)
total_wc2 = merge(total_wc2, wc, by = "word", all = TRUE)
}
total_wc2$freq = rowMeans(total_wc2 %>% select(-word),
na.rm = TRUE)
df$fconnected_s = scale(df$fconnected_s)
total_wc2 = getWC_mat(df$OpenF[1], df$fconnected_s[1])
colnames(total_wc2)[2] = "col1"
getWC_mat(df$OpenF[1], df$fconnected_s[1])
getWC_mat(df$OpenF[1], df$fconnected_s[1])
df$OpenF[1]
df$fconnected_s[1]
df$fconnected_s = scale(df$fconnected_s)
## FB connectedness
df$fconnected_s = scale(df$fconnected)
total_wc2 = getWC_mat(df$OpenF[1], df$fconnected_s[1])
colnames(total_wc2)[2] = "col1"
for (i in c(2:dim(df)[1])){
wc = getWC_mat(df$OpenF[i], df$fconnected_s[i])
colnames(wc)[2] = paste0("col",i)
total_wc2 = merge(total_wc2, wc, by = "word", all = TRUE)
}
total_wc2$freq = rowMeans(total_wc2 %>% select(-word),
na.rm = TRUE)
wordcloud2(data=total_wc2 %>% select(word,freq),
size = 0.7, shape = 'rectangle',
minRotation = 0, maxRotation = 0, rotateRatio = 1)
wordcloud2(data=total_wc %>% select(word,freq),
size = 0.7, shape = 'rectangle',
minRotation = 0, maxRotation = 0, rotateRatio = 1,
hoverFunction = FALSE)
wordcloud2(data=total_wc2 %>% select(word,freq),
size = 0.7, shape = 'rectangle',
minRotation = 0, maxRotation = 0, rotateRatio = 1)
df = read.csv("/Users/Rui/Box Sync/CurrentProjects_Penn/PSAs/05_Analyses/Rui/brain_pe/03_Realcost_datasheets/individualAdLevelData.csv")
df2 = df %>% group_by(pid) %>%
mutate(pe_s = scale(pe0),
share_s = scale(share))
df2
df2$pe_s
View(df2)
df2 = df %>% group_by(pid) %>%
mutate(pe_s = scale(pe0),
share_s = scale(share)) %>%
select(pid, pe0, share)
View(df2)
df = read.csv("/Users/Rui/Box Sync/CurrentProjects_Penn/PSAs/05_Analyses/Rui/brain_pe/03_Realcost_datasheets/individualAdLevelData.csv")
df2 = df %>% group_by(pid) %>%
mutate(pe_s = scale(pe0),
share_s = scale(share)) %>%
select(pid, pe_s, share_s)
View(df2)
sum(df2$pe_s[which(df2$pid == "PSA003")])
scale_this <- function(x){
(x - mean(x, na.rm=TRUE)) / sd(x, na.rm=TRUE)
}
df = read.csv("/Users/Rui/Box Sync/CurrentProjects_Penn/PSAs/05_Analyses/Rui/brain_pe/03_Realcost_datasheets/individualAdLevelData.csv")
scale_this <- function(x){
(x - mean(x, na.rm=TRUE)) / sd(x, na.rm=TRUE)
}
df2 = df %>% group_by(pid) %>%
mutate(pe_s = scale_this(pe0),
share_s = scale_this(share)) %>%
select(pid, pe_s, share_s)
View(df2)
sum(df2$pe_s[which(df2$pid == "PSA003")])
scale_this <- function(x) as.vector(scale(x))
df2 = df %>% group_by(pid) %>%
mutate(pe_s = scale_this(pe0),
share_s = scale_this(share)) %>%
select(pid, pe_s, share_s)
sum(df2$pe_s[which(df2$pid == "PSA003")])
scale_this <- function(x) as.vector(scale(x, center = TRUE))
df2 = df %>% group_by(pid) %>%
mutate(pe_s = scale_this(pe0),
share_s = scale_this(share)) %>%
select(pid, pe_s, share_s)
sum(df2$pe_s[which(df2$pid == "PSA003")])
select(pid, pe_s, share_s)
df2 =
df %>%
group_by(pid) %>%
mutate_all(list(scaled = scale))
df2 =
df %>%
group_by(pid) %>%
mutate_all(list(scaled = scale))
df2 =
df %>%
select(pid, pe0, share) %>%
group_by(pid) %>%
mutate_all(list(scaled = scale))
colnames(df)
df2 =
df %>%
select(pid, pe0, share) %>%
group_by(pid) %>%
mutate_all(list(scaled = scale))
df2
sum(df2$pe0_scaled[which(df2$pid == "PSA003")])
sum(df2$pe0_scaled[which(df2$pid == "PSA005")])
sd(df2$pe0_scaled[which(df2$pid == "PSA005")])
colnames(df2) = c("pid","pe0","share","pe0_s","share_s")
df3 = df2 %>%
group_by(pid) %>%
mutate(pe_ranks = order(order(pe0_s, decreasing=TRUE)),
share_ranks = order(order(share_s, decreasing=TRUE)))
View(df3)
df3 = df2 %>%
group_by(pid) %>%
mutate(pe_ranks = order(pe0_s, decreasing=TRUE),
share_ranks = order(share_s, decreasing=TRUE))
View(df3)
df3 = df2 %>%
group_by(pid) %>%
mutate(pe_ranks = rank(pe0_s, decreasing=TRUE),
share_ranks = rank(share_s, decreasing=TRUE))
df3 = df2 %>%
group_by(pid) %>%
mutate(pe_ranks = rank(pe0_s),
share_ranks = rank(share_s))
View(df3)
detach("package:plyr", unload=TRUE)
df3 = df2 %>%
group_by(pid) %>%
mutate(pe_ranks = rank(pe0_s),
share_ranks = rank(share_s))
View(df3)
df3 = df2 %>%
group_by(pid) %>%
mutate(pe_ranks = order(pe0_s, decreasing = T),
share_ranks = order(share_s, decreasing = T))
View(df3)
df3 = df2 %>%
group_by(pid) %>%
mutate(pe_ranks = order(order(pe0_s, decreasing = T)),
share_ranks = order(order(share_s, decreasing = T)))
View(df3)
df2 =
df %>%
select(pid, vid,pe0, share) %>%
group_by(pid) %>%
mutate_at(c("pe0","share"),list(scaled = scale))
colnames(df2) = c("pid","vid","pe0","share","pe0_s","share_s")
df3 = df2 %>%
group_by(pid) %>%
mutate(pe_ranks = order(order(pe0_s, decreasing = T)),
share_ranks = order(order(share_s, decreasing = T)))
View(df3)
df3$pe_top3 = NA
df3$share_top3 = NA
df3$pe_top3[which(df3$pe_rank < 4)] = 1
df3$pe_top3[which(df3$pe_rank > 9)] = 0
df3$share_top3[which(df3$share_rank < 4)] = 1
df3$share_top3[which(df3$share_rank > 9)] = 0
df3 = df2 %>%
group_by(pid) %>%
mutate(pe_rank = order(order(pe0_s, decreasing = T)),
share_rank = order(order(share_s, decreasing = T)))
df3$pe_top3 = NA
df3$share_top3 = NA
df3$pe_top3[which(df3$pe_rank < 4)] = 1
df3$pe_top3[which(df3$pe_rank > 9)] = 0
df3$share_top3[which(df3$share_rank < 4)] = 1
df3$share_top3[which(df3$share_rank > 9)] = 0
write.csv("/Users/Rui/Desktop/realcost_data.csv")
write.csv(df3,"/Users/Rui/Desktop/realcost_data.csv",
row.names = FALSE)
rm(list = ls())
if (!require("pacman")) install.packages("pacman",repos = "http://cran.us.r-project.org")
pacman::p_load(ggplot2,ggpubr,tidyverse,plyr, dplyr, knitr, kableExtra, jtools,pander)
source("~/Documents/GitHub/facebookUse/functions.R")
pd <- position_dodge(0.3) # move them .05 to the left and right
path =  "~/Box Sync/CurrentProjects_Penn/TPS12_BART/99_fomo/github/"
setwd(path)
path =  "~/Box Sync/CurrentProjects_Penn/TPS12_BART/99_fomo/github (peirui@upenn.edu)/"
setwd(path)
## socialMedia data
socialMedia = read.csv(paste0(path, '0_facebook/tps2_facebook.csv'),na.strings=c("","NA"), stringsAsFactors = FALSE)[-1,]
socialMedia = socialMedia %>%
mutate(TPS_ID = as.factor(TPS_ID),
OpenF = as.factor(OpenF)) %>%
mutate_if(is.character,as.numeric) %>%
mutate(fhab = rowMeans(.[,10:19], na.rm = FALSE),
femo = rowMeans(.[c(8,9,20:39)], na.rm = FALSE),
fconnected = rowMeans(.[c(8,9)], na.rm = FALSE),
femopost = rowMeans(.[c(20:39)], na.rm = FALSE),
TPS_ID = as.character(TPS_ID),
OpenF = as.character(OpenF)) %>%
select(TPS_ID, fhab, femo,fconnected,femopost, OpenF)
cyber = read.csv(paste0(path, '0_cyberball/cyberball_roi.csv'), stringsAsFactors = FALSE)
age = read.csv(paste0(path, '0_surveys/tps12_scanner_study_age.csv'), stringsAsFactors = FALSE) %>%
select(TPS_ID, age, study_t, scanner_t)
=======
tps1 = df %>% filter( study_t == "tps1")
tps2 = df %>% filter( study_t == "tps2")
round(mean(df$age),2)
round(sd(df$age),2)
round(mean(tps1$age),2)
round(sd(tps1$age),2)
round(mean(tps2$age),2)
round(sd(tps2$age),2)
t.test(tps1$age, tps2$age)
cor.test(socialMedia$ConnFam_2, socialMedia$ConnFri_2, method = "pearson")
psych::alpha(socialMedia %>% select(ConnFam_2, ConnFri_2))
stat1 = spssSkewKurtosis(df$fhab)[1,2]/spssSkewKurtosis(df$fhab)[1,1]
g1 = gghistogram(df$fhab, title = "Habitual FB use") +
annotate("text", x=6, y=5, label= paste("stat = ", round(stat1,2)))
round(mean(df$fhab),2)
round(sd(df$fhab),2)
stat2 = spssSkewKurtosis(df$fconnected)[1,2]/spssSkewKurtosis(df$fconnected)[1,1]
g2 = gghistogram(df$fconnected, title = "FB connectedness") +
annotate("text", x=4, y=10, label= paste("stat = ", round(stat2,2)))
round(mean(df$fconnected),2)
round(sd(df$fconnected),2)
stat3 = spssSkewKurtosis(df$IRI_Perspective_Taking)[1,2]/spssSkewKurtosis(df$IRI_Perspective_Taking)[1,1]
g3 = gghistogram(df$IRI_Perspective_Taking, title = "Perspective taking") +
annotate("text", x=15, y=5, label= paste("stat = ", round(stat3,2)))
stat4 = spssSkewKurtosis(df$NTS_Mean)[1,2]/spssSkewKurtosis(df$NTS_Mean)[1,1]
g4 = gghistogram(df$NTS_Mean, title = "NTS") +
annotate("text", x=1.5, y=4, label= paste("stat = ", round(stat3,2)))
multiplot(g1,g3,g2,g4, cols=2)
md1 = lm(fconnected ~ fhab +study_t+ age, data = df)
summ(md1, vifs = TRUE,confint = TRUE, ci.width = 0.95)
ggscatter(df, "fhab", "fconnected")
round(mean(df$fconnected),2)
round(sd(df$fconnected),2)
round(mean(df$fhab),2)
round(sd(df$fhab),2)
md = lm( fconnected~ fhab + age + study_t, data = df)
summ(md, confint = T, scale = F)
md = lm(ado_rois_mask~ fconnected+ scanner_t + study_t + age , data = df)
summ(md, confint = T, scale = F)
md = lm(ado_vs_mask~ fconnected+ scanner_t + study_t + age, data = df)
summ(md, confint = T, scale = F)
md = lm(ado_lateralOFC_mask~ fconnected+ scanner_t + study_t + age, data = df)
summ(md, confint = T, scale = F)
md = lm(saxe_combined_mask~ fconnected+ scanner_t + study_t + age, data = df)
summ(md, confint = T, scale = F)
md = lm(ado_rois_mask~ ConnFri_2+ scanner_t + study_t + age , data = df)
summ(md, confint = T, scale = F)
md = lm(ado_vs_mask~ ConnFri_2+ scanner_t + study_t + age, data = df)
summ(md, confint = T, scale = F)
md = lm(ado_lateralOFC_mask~ ConnFri_2+ scanner_t + study_t + age, data = df)
summ(md, confint = T, scale = F)
md = lm(saxe_combined_mask~ ConnFri_2+ scanner_t + study_t + age, data = df)
summ(md, confint = T, scale = F)
md = lm(NTS_Mean~ ConnFri_2+ scanner_t + study_t + age, data = df)
summ(md, confint = T, scale = F)
md = lm(ado_rois_mask~ ConnFam_2+ scanner_t + study_t + age , data = df)
summ(md, confint = T, scale = F)
md = lm(ado_vs_mask~ ConnFam_2+ scanner_t + study_t + age, data = df)
summ(md, confint = T, scale = F)
md = lm(ado_lateralOFC_mask~ ConnFam_2+ scanner_t + study_t + age, data = df)
summ(md, confint = T, scale = F)
md = lm(saxe_combined_mask~ ConnFam_2+ scanner_t + study_t + age, data = df)
summ(md, confint = T, scale = F)
md = lm(NTS_Mean~ ConnFam_2+ scanner_t + study_t + age, data = df)
summ(md, confint = T, scale = F)
md = lm(ado_rois_mask~ fhab+ scanner_t + study_t + age , data = df)
summ(md, confint = T, scale = F)
md = lm(ado_vs_mask~ fhab+ scanner_t + study_t + age, data = df)
summ(md, confint = T, scale = F)
md = lm(ado_lateralOFC_mask~ fhab+ scanner_t + study_t + age, data = df)
summ(md, confint = T, scale = F)
md = lm(saxe_combined_mask~ fhab+ scanner_t + study_t + age, data = df)
summ(md, confint = T, digits = 3,scale = F)
md = lm(dmpfc_roi_mask~ fhab+ scanner_t + study_t + age, data = df)
summ(md, confint = T, digits = 2,scale = F)
md = lm(mmpfc_roi_mask~ fhab+ scanner_t + study_t + age, data = df)
summ(md, confint = T, digits = 2,scale = F)
md = lm(lTpj_roi_mask~ fhab+ scanner_t + study_t + age, data = df)
summ(md, confint = T, digits = 2,scale = F)
md = lm(rTpj_roi_mask~ fhab+ scanner_t + study_t + age, data = df)
summ(md, confint = T, digits = 3,scale = F)
md1 = lm(NTS_Mean ~ fconnected +study_t +age, data = df)
summ(md1, vifs = TRUE,confint = TRUE, ci.width = 0.95)
md2 = lm(NTS_Mean ~ fhab +study_t +age, data = df)
summ(md2, vifs = TRUE,confint = TRUE, ci.width = 0.95)
md1 = lm(NTS_Mean ~ saxe_combined_mask +study_t + scanner_t +age, data = df)
summ(md1, vifs = TRUE,confint = TRUE, ci.width = 0.95)
md1 = lm(NTS_Mean ~ ado_rois_mask +study_t + scanner_t + age, data = df)
summ(md1, vifs = TRUE,confint = TRUE, ci.width = 0.95)
summary(md5.1)
md5.1 = lm(ado_vs_mask ~ fconnected +study_t +scanner_t + age, data = df)
md5.2 = lm(ado_lateralOFC_mask ~ fconnected +study_t +scanner_t + age, data = df)
export_summs(md5.1, md5.2,scale = FALSE,
error_format = "[{conf.low}, {conf.high}]",
to.file = "xlsx", file.name = "table3.xlsx")
summary(md5.1)
md = lm(ado_rois_mask~ fconnected+ scanner_t + study_t + age , data = df)
summ(md, confint = T, scale = F)
md = lm(saxe_combined_mask~ fconnected+ scanner_t + study_t + age, data = df)
summ(md, confint = T, scale = F)
md = lm(ado_rois_mask~ fhab+ scanner_t + study_t + age , data = df)
summ(md, confint = T, scale = F)
md = lm(saxe_combined_mask~ fhab+ scanner_t + study_t + age, data = df)
summ(md, confint = T, digits = 3,scale = F)
md1 = lm(NTS_Mean ~ fconnected +study_t +age, data = df)
summ(md1, vifs = TRUE,confint = TRUE, ci.width = 0.95)
md2 = lm(NTS_Mean ~ fhab +study_t +age, data = df)
summ(md2, vifs = TRUE,confint = TRUE, ci.width = 0.95)
>>>>>>> Stashed changes
